{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install apache-tvm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SniEOLAcLtBm",
        "outputId": "a2f43333-fc8d-4b8e-9deb-43356a35d125"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting apache-tvm\n",
            "  Downloading apache_tvm-0.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (24.2.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (2.2.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (4.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (1.26.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (5.9.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (1.13.1)\n",
            "Collecting synr==0.6.0 (from apache-tvm)\n",
            "  Downloading synr-0.6.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (6.3.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (4.12.2)\n",
            "Downloading apache_tvm-0.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading synr-0.6.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: synr, apache-tvm\n",
            "Successfully installed apache-tvm-0.11.1 synr-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6Bpp549xJ7A6"
      },
      "outputs": [],
      "source": [
        "import tvm\n",
        "import tvm.testing\n",
        "from tvm import te\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tgt = tvm.target.Target(target=\"llvm\", host=\"llvm\")"
      ],
      "metadata": {
        "id": "NBIVRgvYLdre"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = te.var(\"n\")\n",
        "A = te.placeholder((n,), name=\"A\")\n",
        "B = te.placeholder((n,), name=\"B\")\n",
        "C = te.compute(A.shape, lambda i: A[i] + B[i], name=\"C\")"
      ],
      "metadata": {
        "id": "A8ZhtiOALdt8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = te.create_schedule(C.op)\n",
        "\n",
        "lowered_code = tvm.lower(s, [A, B, C], simple_mode=True)"
      ],
      "metadata": {
        "id": "f24MrLQiLdwo"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fadd = tvm.build(s, [A, B, C], tgt, name=\"myadd\")"
      ],
      "metadata": {
        "id": "G6PwComqLdz9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev = tvm.device(tgt.kind.name, 0)\n",
        "\n",
        "n = 1024\n",
        "a = tvm.nd.array(np.random.uniform(size=n).astype(A.dtype), dev)\n",
        "b = tvm.nd.array(np.random.uniform(size=n).astype(B.dtype), dev)\n",
        "c = tvm.nd.array(np.zeros(n, dtype=C.dtype), dev)\n",
        "fadd(a, b, c)\n",
        "tvm.testing.assert_allclose(c.numpy(), a.numpy() + b.numpy())"
      ],
      "metadata": {
        "id": "c8Y-IwoiMVEK"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import timeit\n",
        "\n",
        "np_repeat = 100\n",
        "np_running_time = timeit.timeit(\n",
        "    setup=\"import numpy\\n\"\n",
        "    \"n = 32768\\n\"\n",
        "    'dtype = \"float32\"\\n'\n",
        "    \"a = numpy.random.rand(n, 1).astype(dtype)\\n\"\n",
        "    \"b = numpy.random.rand(n, 1).astype(dtype)\\n\",\n",
        "    stmt=\"answer = a + b\",\n",
        "    number=np_repeat,\n",
        ")\n",
        "print(\"Numpy running time: %f\" % (np_running_time / np_repeat))\n",
        "\n",
        "\n",
        "def evaluate_addition(func, target, optimization, log):\n",
        "    dev = tvm.device(target.kind.name, 0)\n",
        "    n = 32768\n",
        "    a = tvm.nd.array(np.random.uniform(size=n).astype(A.dtype), dev)\n",
        "    b = tvm.nd.array(np.random.uniform(size=n).astype(B.dtype), dev)\n",
        "    c = tvm.nd.array(np.zeros(n, dtype=C.dtype), dev)\n",
        "\n",
        "    evaluator = func.time_evaluator(func.entry_name, dev, number=10)\n",
        "    mean_time = evaluator(a, b, c).mean\n",
        "    print(\"%s: %f\" % (optimization, mean_time))\n",
        "\n",
        "    log.append((optimization, mean_time))\n",
        "\n",
        "\n",
        "log = [(\"numpy\", np_running_time / np_repeat)]\n",
        "evaluate_addition(fadd, tgt, \"naive\", log=log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ANnxlTYMVHC",
        "outputId": "23a6435e-70a6-4beb-835d-5f91d9031bb5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numpy running time: 0.000013\n",
            "naive: 0.000011\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s[C].parallel(C.op.axis[0])"
      ],
      "metadata": {
        "id": "R0kc3BnhMVIE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tvm.lower(s, [A, B, C], simple_mode=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXJrNxxUMVLv",
        "outputId": "4a272a5b-5b71-4dcd-96d5-2ab2bae904de"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@main = primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
            "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
            "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [n: int32], [stride: int32], type=\"auto\"),\n",
            "             B: Buffer(B_2: Pointer(float32), float32, [n], [stride_1: int32], type=\"auto\"),\n",
            "             C: Buffer(C_2: Pointer(float32), float32, [n], [stride_2: int32], type=\"auto\")}\n",
            "  buffer_map = {A_1: A, B_1: B, C_1: C} {\n",
            "  for (i: int32, 0, n) \"parallel\" {\n",
            "    C_3: Buffer(C_2, float32, [(stride_2*n)], [], type=\"auto\")[(i*stride_2)] = (A_3: Buffer(A_2, float32, [(stride*n)], [], type=\"auto\")[(i*stride)] + B_3: Buffer(B_2, float32, [(stride_1*n)], [], type=\"auto\")[(i*stride_1)])\n",
            "  }\n",
            "}\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fadd_parallel = tvm.build(s, [A, B, C], tgt, name=\"myadd_parallel\")\n",
        "fadd_parallel(a, b, c)\n",
        "\n",
        "tvm.testing.assert_allclose(c.numpy(), a.numpy() + b.numpy())\n",
        "\n",
        "evaluate_addition(fadd_parallel, tgt, \"parallel\", log=log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKUx2L8GMVQR",
        "outputId": "1c7bc1af-3c31-4951-f114-3cc0edc2259f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "parallel: 0.000025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Recreate the schedule, since we modified it with the parallel operation in\n",
        "# the previous example\n",
        "n = te.var(\"n\")\n",
        "A = te.placeholder((n,), name=\"A\")\n",
        "B = te.placeholder((n,), name=\"B\")\n",
        "C = te.compute(A.shape, lambda i: A[i] + B[i], name=\"C\")\n",
        "\n",
        "s = te.create_schedule(C.op)\n",
        "\n",
        "# This factor should be chosen to match the number of threads appropriate for\n",
        "# your CPU. This will vary depending on architecture, but a good rule is\n",
        "# setting this factor to equal the number of available CPU cores.\n",
        "factor = 4\n",
        "\n",
        "outer, inner = s[C].split(C.op.axis[0], factor=factor)\n",
        "s[C].parallel(outer)\n",
        "s[C].vectorize(inner)\n",
        "\n",
        "fadd_vector = tvm.build(s, [A, B, C], tgt, name=\"myadd_parallel\")\n",
        "\n",
        "evaluate_addition(fadd_vector, tgt, \"vector\", log=log)\n",
        "\n",
        "print(tvm.lower(s, [A, B, C], simple_mode=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7__2c-pLd3T",
        "outputId": "a8355b7a-ad8c-40dd-dca5-8f745926c69f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vector: 0.000045\n",
            "@main = primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
            "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
            "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [n: int32], [stride: int32], type=\"auto\"),\n",
            "             B: Buffer(B_2: Pointer(float32), float32, [n], [stride_1: int32], type=\"auto\"),\n",
            "             C: Buffer(C_2: Pointer(float32), float32, [n], [stride_2: int32], type=\"auto\")}\n",
            "  buffer_map = {A_1: A, B_1: B, C_1: C} {\n",
            "  for (i.outer: int32, 0, floordiv((n + 3), 4)) \"parallel\" {\n",
            "    for (i.inner.s: int32, 0, 4) {\n",
            "      if @tir.likely((((i.outer*4) + i.inner.s) < n), dtype=bool) {\n",
            "        let cse_var_1: int32 = ((i.outer*4) + i.inner.s)\n",
            "        C_3: Buffer(C_2, float32, [(stride_2*n)], [], type=\"auto\")[(cse_var_1*stride_2)] = (A_3: Buffer(A_2, float32, [(stride*n)], [], type=\"auto\")[(cse_var_1*stride)] + B_3: Buffer(B_2, float32, [(stride_1*n)], [], type=\"auto\")[(cse_var_1*stride_1)])\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baseline = log[0][1]\n",
        "print(\"%s\\t%s\\t%s\" % (\"Operator\".rjust(20), \"Timing\".rjust(20), \"Performance\".rjust(20)))\n",
        "for result in log:\n",
        "    print(\n",
        "        \"%s\\t%s\\t%s\"\n",
        "        % (result[0].rjust(20), str(result[1]).rjust(20), str(result[1] / baseline).rjust(20))\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkxMAbyXNNbp",
        "outputId": "c64cfc22-f3ea-42ba-d341-4db19c04f550"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Operator\t              Timing\t         Performance\n",
            "               numpy\t1.2562330000491784e-05\t                 1.0\n",
            "               naive\t         1.13886e-05\t  0.9065674918231064\n",
            "            parallel\t2.5028699999999996e-05\t  1.9923612895872171\n",
            "              vector\t         4.48296e-05\t  3.5685736641407315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tvm.contrib import cc\n",
        "from tvm.contrib import utils\n",
        "\n",
        "temp = utils.tempdir()\n",
        "fadd.save(temp.relpath(\"myadd.o\"))\n",
        "if tgt.kind.name == \"cuda\":\n",
        "    fadd.imported_modules[0].save(temp.relpath(\"myadd.ptx\"))\n",
        "if tgt.kind.name == \"rocm\":\n",
        "    fadd.imported_modules[0].save(temp.relpath(\"myadd.hsaco\"))\n",
        "if tgt.kind.name.startswith(\"opencl\"):\n",
        "    fadd.imported_modules[0].save(temp.relpath(\"myadd.cl\"))\n",
        "cc.create_shared(temp.relpath(\"myadd.so\"), [temp.relpath(\"myadd.o\")])\n",
        "print(temp.listdir())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTjA_es6NNeG",
        "outputId": "505b1d9a-7fb7-45f3-b6a8-cdf6ce81bdba"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['myadd.o', 'myadd.so']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fadd1 = tvm.runtime.load_module(temp.relpath(\"myadd.so\"))\n",
        "if tgt.kind.name == \"cuda\":\n",
        "    fadd1_dev = tvm.runtime.load_module(temp.relpath(\"myadd.ptx\"))\n",
        "    fadd1.import_module(fadd1_dev)\n",
        "\n",
        "if tgt.kind.name == \"rocm\":\n",
        "    fadd1_dev = tvm.runtime.load_module(temp.relpath(\"myadd.hsaco\"))\n",
        "    fadd1.import_module(fadd1_dev)\n",
        "\n",
        "if tgt.kind.name.startswith(\"opencl\"):\n",
        "    fadd1_dev = tvm.runtime.load_module(temp.relpath(\"myadd.cl\"))\n",
        "    fadd1.import_module(fadd1_dev)\n",
        "\n",
        "fadd1(a, b, c)\n",
        "tvm.testing.assert_allclose(c.numpy(), a.numpy() + b.numpy())"
      ],
      "metadata": {
        "id": "XmZck5fMNNgr"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fadd.export_library(temp.relpath(\"myadd_pack.so\"))\n",
        "fadd2 = tvm.runtime.load_module(temp.relpath(\"myadd_pack.so\"))\n",
        "fadd2(a, b, c)\n",
        "tvm.testing.assert_allclose(c.numpy(), a.numpy() + b.numpy())"
      ],
      "metadata": {
        "id": "W6DqD-l9NNjy"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tgt.kind.name.startswith(\"opencl\"):\n",
        "    fadd_cl = tvm.build(s, [A, B, C], tgt, name=\"myadd\")\n",
        "    print(\"------opencl code------\")\n",
        "    print(fadd_cl.imported_modules[0].get_source())\n",
        "    dev = tvm.cl(0)\n",
        "    n = 1024\n",
        "    a = tvm.nd.array(np.random.uniform(size=n).astype(A.dtype), dev)\n",
        "    b = tvm.nd.array(np.random.uniform(size=n).astype(B.dtype), dev)\n",
        "    c = tvm.nd.array(np.zeros(n, dtype=C.dtype), dev)\n",
        "    fadd_cl(a, b, c)\n",
        "    tvm.testing.assert_allclose(c.numpy(), a.numpy() + b.numpy())"
      ],
      "metadata": {
        "id": "XxZoxHUtNeGX"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Manually Optimizing Matrix Multiplication with TE"
      ],
      "metadata": {
        "id": "5E_0YFZeNeJK"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tvm\n",
        "import tvm.testing\n",
        "from tvm import te\n",
        "import numpy\n",
        "\n",
        "# The size of the matrix\n",
        "# (M, K) x (K, N)\n",
        "# You are free to try out different shapes, sometimes TVM optimization outperforms numpy with MKL.\n",
        "M = 1024\n",
        "K = 1024\n",
        "N = 1024\n",
        "\n",
        "# The default tensor data type in tvm\n",
        "dtype = \"float32\"\n",
        "\n",
        "# You will want to adjust the target to match any CPU vector extensions you\n",
        "# might have. For example, if you're using using Intel AVX2 (Advanced Vector\n",
        "# Extensions) ISA for SIMD, you can get the best performance by changing the\n",
        "# following line to ``llvm -mcpu=core-avx2``, or specific type of CPU you use.\n",
        "# Recall that you're using llvm, you can get this information from the command\n",
        "# ``llc --version`` to get the CPU type, and you can check ``/proc/cpuinfo``\n",
        "# for additional extensions that your processor might support.\n",
        "\n",
        "target = tvm.target.Target(target=\"llvm\", host=\"llvm\")\n",
        "dev = tvm.device(target.kind.name, 0)\n",
        "\n",
        "# Random generated tensor for testing\n",
        "a = tvm.nd.array(numpy.random.rand(M, K).astype(dtype), dev)\n",
        "b = tvm.nd.array(numpy.random.rand(K, N).astype(dtype), dev)\n",
        "\n",
        "# Repeatedly perform a matrix multiplication to get a performance baseline\n",
        "# for the default numpy implementation\n",
        "np_repeat = 100\n",
        "np_running_time = timeit.timeit(\n",
        "    setup=\"import numpy\\n\"\n",
        "    \"M = \" + str(M) + \"\\n\"\n",
        "    \"K = \" + str(K) + \"\\n\"\n",
        "    \"N = \" + str(N) + \"\\n\"\n",
        "    'dtype = \"float32\"\\n'\n",
        "    \"a = numpy.random.rand(M, K).astype(dtype)\\n\"\n",
        "    \"b = numpy.random.rand(K, N).astype(dtype)\\n\",\n",
        "    stmt=\"answer = numpy.dot(a, b)\",\n",
        "    number=np_repeat,\n",
        ")\n",
        "print(\"Numpy running time: %f\" % (np_running_time / np_repeat))\n",
        "\n",
        "answer = numpy.dot(a.numpy(), b.numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQx0DBoeNeL2",
        "outputId": "59eee35a-999d-4ebf-919c-dbfb20e08a72"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numpy running time: 0.034227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TVM Matrix Multiplication using TE\n",
        "k = te.reduce_axis((0, K), \"k\")\n",
        "A = te.placeholder((M, K), name=\"A\")\n",
        "B = te.placeholder((K, N), name=\"B\")\n",
        "C = te.compute((M, N), lambda x, y: te.sum(A[x, k] * B[k, y], axis=k), name=\"C\")\n",
        "\n",
        "# Default schedule\n",
        "s = te.create_schedule(C.op)\n",
        "func = tvm.build(s, [A, B, C], target=target, name=\"mmult\")\n",
        "\n",
        "c = tvm.nd.array(numpy.zeros((M, N), dtype=dtype), dev)\n",
        "func(a, b, c)\n",
        "tvm.testing.assert_allclose(c.numpy(), answer, rtol=1e-5)\n",
        "\n",
        "\n",
        "def evaluate_operation(s, vars, target, name, optimization, log):\n",
        "    func = tvm.build(s, [A, B, C], target=target, name=\"mmult\")\n",
        "    assert func\n",
        "\n",
        "    c = tvm.nd.array(numpy.zeros((M, N), dtype=dtype), dev)\n",
        "    func(a, b, c)\n",
        "    tvm.testing.assert_allclose(c.numpy(), answer, rtol=1e-5)\n",
        "\n",
        "    evaluator = func.time_evaluator(func.entry_name, dev, number=10)\n",
        "    mean_time = evaluator(a, b, c).mean\n",
        "    print(\"%s: %f\" % (optimization, mean_time))\n",
        "    log.append((optimization, mean_time))\n",
        "\n",
        "\n",
        "log = []\n",
        "\n",
        "evaluate_operation(s, [A, B, C], target=target, name=\"mmult\", optimization=\"none\", log=log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zouOpum_NeO3",
        "outputId": "fe6fbf07-a804-4ede-eb0e-562db5f4c9be"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "none: 4.140988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tvm.lower(s, [A, B, C], simple_mode=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ce6w4Jo_NeSd",
        "outputId": "644d2a62-77f5-442b-e871-1a1fa280577b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@main = primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
            "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
            "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [1024, 1024], []),\n",
            "             B: Buffer(B_2: Pointer(float32), float32, [1024, 1024], []),\n",
            "             C: Buffer(C_2: Pointer(float32), float32, [1024, 1024], [])}\n",
            "  buffer_map = {A_1: A, B_1: B, C_1: C} {\n",
            "  for (x: int32, 0, 1024) {\n",
            "    for (y: int32, 0, 1024) {\n",
            "      C_3: Buffer(C_2, float32, [1048576], [])[((x*1024) + y)] = 0f32\n",
            "      for (k: int32, 0, 1024) {\n",
            "        let cse_var_2: int32 = (x*1024)\n",
            "        let cse_var_1: int32 = (cse_var_2 + y)\n",
            "        C_3[cse_var_1] = (C_3[cse_var_1] + (A_3: Buffer(A_2, float32, [1048576], [])[(cse_var_2 + k)]*B_3: Buffer(B_2, float32, [1048576], [])[((k*1024) + y)]))\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bn = 32\n",
        "\n",
        "# Blocking by loop tiling\n",
        "xo, yo, xi, yi = s[C].tile(C.op.axis[0], C.op.axis[1], bn, bn)\n",
        "(k,) = s[C].op.reduce_axis\n",
        "ko, ki = s[C].split(k, factor=4)\n",
        "\n",
        "# Hoist reduction domain outside the blocking loop\n",
        "s[C].reorder(xo, yo, ko, ki, xi, yi)\n",
        "\n",
        "evaluate_operation(s, [A, B, C], target=target, name=\"mmult\", optimization=\"blocking\", log=log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Mw7C2aMNNmZ",
        "outputId": "d75d5cf7-f983-4bae-bb52-5111871971aa"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "blocking: 0.394628\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tvm.lower(s, [A, B, C], simple_mode=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhgw8DcxNNp0",
        "outputId": "6e444f6a-3c42-4bfd-b1ce-ff5c3c164cc1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@main = primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
            "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
            "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [1024, 1024], []),\n",
            "             B: Buffer(B_2: Pointer(float32), float32, [1024, 1024], []),\n",
            "             C: Buffer(C_2: Pointer(float32), float32, [1024, 1024], [])}\n",
            "  buffer_map = {A_1: A, B_1: B, C_1: C} {\n",
            "  for (x.outer: int32, 0, 32) {\n",
            "    for (y.outer: int32, 0, 32) {\n",
            "      for (x.inner.init: int32, 0, 32) {\n",
            "        for (y.inner.init: int32, 0, 32) {\n",
            "          C_3: Buffer(C_2, float32, [1048576], [])[((((x.outer*32768) + (x.inner.init*1024)) + (y.outer*32)) + y.inner.init)] = 0f32\n",
            "        }\n",
            "      }\n",
            "      for (k.outer: int32, 0, 256) {\n",
            "        for (k.inner: int32, 0, 4) {\n",
            "          for (x.inner: int32, 0, 32) {\n",
            "            for (y.inner: int32, 0, 32) {\n",
            "              let cse_var_3: int32 = (y.outer*32)\n",
            "              let cse_var_2: int32 = ((x.outer*32768) + (x.inner*1024))\n",
            "              let cse_var_1: int32 = ((cse_var_2 + cse_var_3) + y.inner)\n",
            "              C_3[cse_var_1] = (C_3[cse_var_1] + (A_3: Buffer(A_2, float32, [1048576], [])[((cse_var_2 + (k.outer*4)) + k.inner)]*B_3: Buffer(B_2, float32, [1048576], [])[((((k.outer*4096) + (k.inner*1024)) + cse_var_3) + y.inner)]))\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the vectorization optimization\n",
        "s[C].vectorize(yi)\n",
        "\n",
        "evaluate_operation(s, [A, B, C], target=target, name=\"mmult\", optimization=\"vectorization\", log=log)\n",
        "\n",
        "# The generalized IR after vectorization\n",
        "print(tvm.lower(s, [A, B, C], simple_mode=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OF89WaPGODKt",
        "outputId": "06a499eb-b27c-418a-c5b8-cc501ea687c6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vectorization: 0.407555\n",
            "@main = primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
            "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
            "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [1024, 1024], []),\n",
            "             B: Buffer(B_2: Pointer(float32), float32, [1024, 1024], []),\n",
            "             C: Buffer(C_2: Pointer(float32), float32, [1024, 1024], [])}\n",
            "  buffer_map = {A_1: A, B_1: B, C_1: C} {\n",
            "  for (x.outer: int32, 0, 32) {\n",
            "    for (y.outer: int32, 0, 32) {\n",
            "      for (x.inner.init: int32, 0, 32) {\n",
            "        C_3: Buffer(C_2, float32, [1048576], [])[ramp((((x.outer*32768) + (x.inner.init*1024)) + (y.outer*32)), 1, 32)] = broadcast(0f32, 32)\n",
            "      }\n",
            "      for (k.outer: int32, 0, 256) {\n",
            "        for (k.inner: int32, 0, 4) {\n",
            "          for (x.inner: int32, 0, 32) {\n",
            "            let cse_var_3: int32 = (y.outer*32)\n",
            "            let cse_var_2: int32 = ((x.outer*32768) + (x.inner*1024))\n",
            "            let cse_var_1: int32 = (cse_var_2 + cse_var_3)\n",
            "            C_3[ramp(cse_var_1, 1, 32)] = (C_3[ramp(cse_var_1, 1, 32)] + (broadcast(A_3: Buffer(A_2, float32, [1048576], [])[((cse_var_2 + (k.outer*4)) + k.inner)], 32)*B_3: Buffer(B_2, float32, [1048576], [])[ramp((((k.outer*4096) + (k.inner*1024)) + cse_var_3), 1, 32)]))\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = te.create_schedule(C.op)\n",
        "xo, yo, xi, yi = s[C].tile(C.op.axis[0], C.op.axis[1], bn, bn)\n",
        "(k,) = s[C].op.reduce_axis\n",
        "ko, ki = s[C].split(k, factor=4)\n",
        "\n",
        "# re-ordering\n",
        "s[C].reorder(xo, yo, ko, xi, ki, yi)\n",
        "s[C].vectorize(yi)\n",
        "\n",
        "evaluate_operation(\n",
        "    s, [A, B, C], target=target, name=\"mmult\", optimization=\"loop permutation\", log=log\n",
        ")\n",
        "\n",
        "# Again, print the new generalized IR\n",
        "print(tvm.lower(s, [A, B, C], simple_mode=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3E_YNuYODLn",
        "outputId": "af10d846-78c7-4d42-dcd3-5d39d784c944"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loop permutation: 0.166295\n",
            "@main = primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
            "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
            "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [1024, 1024], []),\n",
            "             B: Buffer(B_2: Pointer(float32), float32, [1024, 1024], []),\n",
            "             C: Buffer(C_2: Pointer(float32), float32, [1024, 1024], [])}\n",
            "  buffer_map = {A_1: A, B_1: B, C_1: C} {\n",
            "  for (x.outer: int32, 0, 32) {\n",
            "    for (y.outer: int32, 0, 32) {\n",
            "      for (x.inner.init: int32, 0, 32) {\n",
            "        C_3: Buffer(C_2, float32, [1048576], [])[ramp((((x.outer*32768) + (x.inner.init*1024)) + (y.outer*32)), 1, 32)] = broadcast(0f32, 32)\n",
            "      }\n",
            "      for (k.outer: int32, 0, 256) {\n",
            "        for (x.inner: int32, 0, 32) {\n",
            "          for (k.inner: int32, 0, 4) {\n",
            "            let cse_var_3: int32 = (y.outer*32)\n",
            "            let cse_var_2: int32 = ((x.outer*32768) + (x.inner*1024))\n",
            "            let cse_var_1: int32 = (cse_var_2 + cse_var_3)\n",
            "            C_3[ramp(cse_var_1, 1, 32)] = (C_3[ramp(cse_var_1, 1, 32)] + (broadcast(A_3: Buffer(A_2, float32, [1048576], [])[((cse_var_2 + (k.outer*4)) + k.inner)], 32)*B_3: Buffer(B_2, float32, [1048576], [])[ramp((((k.outer*4096) + (k.inner*1024)) + cse_var_3), 1, 32)]))\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We have to re-write the algorithm slightly.\n",
        "packedB = te.compute((N / bn, K, bn), lambda x, y, z: B[y, x * bn + z], name=\"packedB\")\n",
        "C = te.compute(\n",
        "    (M, N),\n",
        "    lambda x, y: te.sum(A[x, k] * packedB[y // bn, k, tvm.tir.indexmod(y, bn)], axis=k),\n",
        "    name=\"C\",\n",
        ")\n",
        "\n",
        "s = te.create_schedule(C.op)\n",
        "\n",
        "xo, yo, xi, yi = s[C].tile(C.op.axis[0], C.op.axis[1], bn, bn)\n",
        "(k,) = s[C].op.reduce_axis\n",
        "ko, ki = s[C].split(k, factor=4)\n",
        "\n",
        "s[C].reorder(xo, yo, ko, xi, ki, yi)\n",
        "s[C].vectorize(yi)\n",
        "\n",
        "x, y, z = s[packedB].op.axis\n",
        "s[packedB].vectorize(z)\n",
        "s[packedB].parallel(x)\n",
        "\n",
        "evaluate_operation(s, [A, B, C], target=target, name=\"mmult\", optimization=\"array packing\", log=log)\n",
        "\n",
        "# Here is the generated IR after array packing.\n",
        "print(tvm.lower(s, [A, B, C], simple_mode=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geJrbZKwODMe",
        "outputId": "929fa025-e29a-425f-ffa6-edc3050e8981"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "array packing: 0.155161\n",
            "@main = primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
            "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
            "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [1024, 1024], []),\n",
            "             B: Buffer(B_2: Pointer(float32), float32, [1024, 1024], []),\n",
            "             C: Buffer(C_2: Pointer(float32), float32, [1024, 1024], [])}\n",
            "  buffer_map = {A_1: A, B_1: B, C_1: C} {\n",
            "  allocate(packedB: Pointer(global float32x32), float32x32, [32768]), storage_scope = global {\n",
            "    for (x: int32, 0, 32) \"parallel\" {\n",
            "      for (y: int32, 0, 1024) {\n",
            "        packedB_1: Buffer(packedB, float32x32, [32768], [])[((x*1024) + y)] = B_3: Buffer(B_2, float32, [1048576], [])[ramp(((y*1024) + (x*32)), 1, 32)]\n",
            "      }\n",
            "    }\n",
            "    for (x.outer: int32, 0, 32) {\n",
            "      for (y.outer: int32, 0, 32) {\n",
            "        for (x.inner.init: int32, 0, 32) {\n",
            "          C_3: Buffer(C_2, float32, [1048576], [])[ramp((((x.outer*32768) + (x.inner.init*1024)) + (y.outer*32)), 1, 32)] = broadcast(0f32, 32)\n",
            "        }\n",
            "        for (k.outer: int32, 0, 256) {\n",
            "          for (x.inner: int32, 0, 32) {\n",
            "            for (k.inner: int32, 0, 4) {\n",
            "              let cse_var_3: int32 = ((x.outer*32768) + (x.inner*1024))\n",
            "              let cse_var_2: int32 = (k.outer*4)\n",
            "              let cse_var_1: int32 = (cse_var_3 + (y.outer*32))\n",
            "              C_3[ramp(cse_var_1, 1, 32)] = (C_3[ramp(cse_var_1, 1, 32)] + (broadcast(A_3: Buffer(A_2, float32, [1048576], [])[((cse_var_3 + cse_var_2) + k.inner)], 32)*packedB_1[(((y.outer*1024) + cse_var_2) + k.inner)]))\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = te.create_schedule(C.op)\n",
        "\n",
        "# Allocate write cache\n",
        "CC = s.cache_write(C, \"global\")\n",
        "\n",
        "xo, yo, xi, yi = s[C].tile(C.op.axis[0], C.op.axis[1], bn, bn)\n",
        "\n",
        "# Write cache is computed at yo\n",
        "s[CC].compute_at(s[C], yo)\n",
        "\n",
        "# New inner axes\n",
        "xc, yc = s[CC].op.axis\n",
        "\n",
        "(k,) = s[CC].op.reduce_axis\n",
        "ko, ki = s[CC].split(k, factor=4)\n",
        "s[CC].reorder(ko, xc, ki, yc)\n",
        "s[CC].unroll(ki)\n",
        "s[CC].vectorize(yc)\n",
        "\n",
        "x, y, z = s[packedB].op.axis\n",
        "s[packedB].vectorize(z)\n",
        "s[packedB].parallel(x)\n",
        "\n",
        "evaluate_operation(s, [A, B, C], target=target, name=\"mmult\", optimization=\"block caching\", log=log)\n",
        "\n",
        "# Here is the generated IR after write cache blocking.\n",
        "print(tvm.lower(s, [A, B, C], simple_mode=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EqNu5aVODPB",
        "outputId": "f7a38f90-9e9a-4857-bd56-f8765d6ffae8"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "block caching: 0.123341\n",
            "@main = primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
            "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
            "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [1024, 1024], []),\n",
            "             B: Buffer(B_2: Pointer(float32), float32, [1024, 1024], []),\n",
            "             C: Buffer(C_2: Pointer(float32), float32, [1024, 1024], [])}\n",
            "  buffer_map = {A_1: A, B_1: B, C_1: C} {\n",
            "  allocate(packedB: Pointer(global float32x32), float32x32, [32768]), storage_scope = global;\n",
            "  allocate(C.global: Pointer(global float32), float32, [1024]), storage_scope = global {\n",
            "    for (x: int32, 0, 32) \"parallel\" {\n",
            "      for (y: int32, 0, 1024) {\n",
            "        packedB_1: Buffer(packedB, float32x32, [32768], [])[((x*1024) + y)] = B_3: Buffer(B_2, float32, [1048576], [])[ramp(((y*1024) + (x*32)), 1, 32)]\n",
            "      }\n",
            "    }\n",
            "    for (x.outer: int32, 0, 32) {\n",
            "      for (y.outer: int32, 0, 32) {\n",
            "        for (x.c.init: int32, 0, 32) {\n",
            "          C.global_1: Buffer(C.global, float32, [1024], [])[ramp((x.c.init*32), 1, 32)] = broadcast(0f32, 32)\n",
            "        }\n",
            "        for (k.outer: int32, 0, 256) {\n",
            "          for (x.c: int32, 0, 32) {\n",
            "            let cse_var_4: int32 = (k.outer*4)\n",
            "            let cse_var_3: int32 = (x.c*32)\n",
            "            let cse_var_2: int32 = ((y.outer*1024) + cse_var_4)\n",
            "            let cse_var_1: int32 = (((x.outer*32768) + (x.c*1024)) + cse_var_4)\n",
            "             {\n",
            "              C.global_1[ramp(cse_var_3, 1, 32)] = (C.global_1[ramp(cse_var_3, 1, 32)] + (broadcast(A_3: Buffer(A_2, float32, [1048576], [])[cse_var_1], 32)*packedB_1[cse_var_2]))\n",
            "              C.global_1[ramp(cse_var_3, 1, 32)] = (C.global_1[ramp(cse_var_3, 1, 32)] + (broadcast(A_3[(cse_var_1 + 1)], 32)*packedB_1[(cse_var_2 + 1)]))\n",
            "              C.global_1[ramp(cse_var_3, 1, 32)] = (C.global_1[ramp(cse_var_3, 1, 32)] + (broadcast(A_3[(cse_var_1 + 2)], 32)*packedB_1[(cse_var_2 + 2)]))\n",
            "              C.global_1[ramp(cse_var_3, 1, 32)] = (C.global_1[ramp(cse_var_3, 1, 32)] + (broadcast(A_3[(cse_var_1 + 3)], 32)*packedB_1[(cse_var_2 + 3)]))\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        for (x.inner: int32, 0, 32) {\n",
            "          for (y.inner: int32, 0, 32) {\n",
            "            C_3: Buffer(C_2, float32, [1048576], [])[((((x.outer*32768) + (x.inner*1024)) + (y.outer*32)) + y.inner)] = C.global_1[((x.inner*32) + y.inner)]\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# parallel\n",
        "s[C].parallel(xo)\n",
        "\n",
        "x, y, z = s[packedB].op.axis\n",
        "s[packedB].vectorize(z)\n",
        "s[packedB].parallel(x)\n",
        "\n",
        "evaluate_operation(\n",
        "    s, [A, B, C], target=target, name=\"mmult\", optimization=\"parallelization\", log=log\n",
        ")\n",
        "\n",
        "# Here is the generated IR after parallelization.\n",
        "print(tvm.lower(s, [A, B, C], simple_mode=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKyO4p-gODRC",
        "outputId": "fdc51a56-196b-45eb-88c6-d1b5fb521119"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "parallelization: 0.157714\n",
            "@main = primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
            "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
            "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [1024, 1024], []),\n",
            "             B: Buffer(B_2: Pointer(float32), float32, [1024, 1024], []),\n",
            "             C: Buffer(C_2: Pointer(float32), float32, [1024, 1024], [])}\n",
            "  buffer_map = {A_1: A, B_1: B, C_1: C} {\n",
            "  allocate(packedB: Pointer(global float32x32), float32x32, [32768]), storage_scope = global {\n",
            "    for (x: int32, 0, 32) \"parallel\" {\n",
            "      for (y: int32, 0, 1024) {\n",
            "        packedB_1: Buffer(packedB, float32x32, [32768], [])[((x*1024) + y)] = B_3: Buffer(B_2, float32, [1048576], [])[ramp(((y*1024) + (x*32)), 1, 32)]\n",
            "      }\n",
            "    }\n",
            "    for (x.outer: int32, 0, 32) \"parallel\" {\n",
            "      allocate(C.global: Pointer(global float32), float32, [1024]), storage_scope = global;\n",
            "      for (y.outer: int32, 0, 32) {\n",
            "        for (x.c.init: int32, 0, 32) {\n",
            "          C.global_1: Buffer(C.global, float32, [1024], [])[ramp((x.c.init*32), 1, 32)] = broadcast(0f32, 32)\n",
            "        }\n",
            "        for (k.outer: int32, 0, 256) {\n",
            "          for (x.c: int32, 0, 32) {\n",
            "            let cse_var_4: int32 = (k.outer*4)\n",
            "            let cse_var_3: int32 = (x.c*32)\n",
            "            let cse_var_2: int32 = ((y.outer*1024) + cse_var_4)\n",
            "            let cse_var_1: int32 = (((x.outer*32768) + (x.c*1024)) + cse_var_4)\n",
            "             {\n",
            "              C.global_1[ramp(cse_var_3, 1, 32)] = (C.global_1[ramp(cse_var_3, 1, 32)] + (broadcast(A_3: Buffer(A_2, float32, [1048576], [])[cse_var_1], 32)*packedB_1[cse_var_2]))\n",
            "              C.global_1[ramp(cse_var_3, 1, 32)] = (C.global_1[ramp(cse_var_3, 1, 32)] + (broadcast(A_3[(cse_var_1 + 1)], 32)*packedB_1[(cse_var_2 + 1)]))\n",
            "              C.global_1[ramp(cse_var_3, 1, 32)] = (C.global_1[ramp(cse_var_3, 1, 32)] + (broadcast(A_3[(cse_var_1 + 2)], 32)*packedB_1[(cse_var_2 + 2)]))\n",
            "              C.global_1[ramp(cse_var_3, 1, 32)] = (C.global_1[ramp(cse_var_3, 1, 32)] + (broadcast(A_3[(cse_var_1 + 3)], 32)*packedB_1[(cse_var_2 + 3)]))\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        for (x.inner: int32, 0, 32) {\n",
            "          for (y.inner: int32, 0, 32) {\n",
            "            C_3: Buffer(C_2, float32, [1048576], [])[((((x.outer*32768) + (x.inner*1024)) + (y.outer*32)) + y.inner)] = C.global_1[((x.inner*32) + y.inner)]\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baseline = log[0][1]\n",
        "print(\"%s\\t%s\\t%s\" % (\"Operator\".rjust(20), \"Timing\".rjust(20), \"Performance\".rjust(20)))\n",
        "for result in log:\n",
        "    print(\n",
        "        \"%s\\t%s\\t%s\"\n",
        "        % (result[0].rjust(20), str(result[1]).rjust(20), str(result[1] / baseline).rjust(20))\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRemaDadODTO",
        "outputId": "33ce1c98-f1e6-47ff-927b-2c515635bc8d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Operator\t              Timing\t         Performance\n",
            "                none\t        4.1409875947\t                 1.0\n",
            "            blocking\t        0.3946275962\t 0.09529794213947394\n",
            "       vectorization\t        0.4075554807\t 0.09841987481962643\n",
            "    loop permutation\t        0.1662953322\t0.040158374879663825\n",
            "       array packing\t        0.1551607974\t 0.03746951514623913\n",
            "       block caching\t 0.12334071819999999\t 0.02978533873365433\n",
            "     parallelization\t        0.1577135657\t0.038085978789662564\n"
          ]
        }
      ]
    }
  ]
}